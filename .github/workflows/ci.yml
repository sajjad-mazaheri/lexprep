# CI Pipeline for lexprep


name: CI

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # Lint 
  lint:
    name: Lint & Type Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-lint-${{ hashFiles('pyproject.toml') }}
          restore-keys: pip-lint-

      - name: Install dev dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Ruff check
        run: ruff check src/ tests/

      - name: Ruff format check
        run: ruff format --check src/ tests/
        continue-on-error: true

      - name: Mypy
        run: mypy src/ --ignore-missing-imports
        continue-on-error: true

  # Core tests (no NLP deps)
  test-core:
    name: Core Tests (${{ matrix.os }} / py${{ matrix.python-version }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest]
        python-version: ["3.10", "3.12"]

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~\AppData\Local\pip\Cache
          key: pip-core-${{ matrix.os }}-py${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: pip-core-${{ matrix.os }}-py${{ matrix.python-version }}-

      - name: Install core + dev dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run core tests
        run: |
          pytest tests/test_io_files.py tests/test_stratified.py tests/test_stratified_full.py tests/test_shuffle.py tests/test_fa_syllables.py tests/test_ja_pos_map.py tests/test_cli.py -v --tb=short

  #  Web API tests (with all NLP deps for full coverage) 
  test-web:
    name: Web API Tests (All Tools)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-web-all-${{ hashFiles('pyproject.toml') }}
          restore-keys: pip-web-all-

      - name: Cache NLP models
        uses: actions/cache@v4
        with:
          path: |
            ~/stanza_resources
            ~/.cache/spacy
          key: nlp-models-web-v1
          restore-keys: nlp-models-web-

      - name: Install all dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,en,fa,ja]"
          pip install flask flask-cors markupsafe
          python -m spacy download en_core_web_sm

      - name: Download NLTK data
        run: python -c "import nltk; nltk.download('averaged_perceptron_tagger_eng'); nltk.download('cmudict')"

      - name: Download Stanza models
        run: |
          python -c "import stanza; stanza.download('fa', verbose=False)"
          python -c "import stanza; stanza.download('ja', verbose=False)"

      - name: Run web API tests
        run: |
          pytest tests/test_web_api.py -v --tb=short

  #  NLP integration tests (English) 
  test-nlp-en:
    name: English NLP Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-en-${{ hashFiles('pyproject.toml') }}
          restore-keys: pip-en-

      - name: Cache spaCy model
        uses: actions/cache@v4
        with:
          path: ~/.cache/spacy
          key: spacy-en-core-web-sm-v1
          restore-keys: spacy-en-

      - name: Install English NLP deps
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,en]"
          python -m spacy download en_core_web_sm

      - name: Download NLTK data
        run: python -c "import nltk; nltk.download('averaged_perceptron_tagger_eng'); nltk.download('cmudict')"

      - name: Run English tests
        run: |
          pytest tests/test_en_syllables.py tests/test_en_g2p.py tests/test_en_pos.py -v --tb=short

  #  NLP integration tests (Persian) 
  test-nlp-fa:
    name: Persian NLP Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-fa-${{ hashFiles('pyproject.toml') }}
          restore-keys: pip-fa-

      - name: Cache Stanza Persian model
        uses: actions/cache@v4
        with:
          path: ~/stanza_resources
          key: stanza-fa-v1
          restore-keys: stanza-fa-

      - name: Install Persian NLP deps
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,fa]"

      - name: Download Stanza Persian model
        run: |
          python -c "import stanza; stanza.download('fa', verbose=False)"

      - name: Run Persian tests
        run: |
          pytest tests/test_fa_g2p.py tests/test_fa_pos.py tests/test_fa_syllables.py -v --tb=short

  #  NLP integration tests (Japanese) 
  test-nlp-ja:
    name: Japanese NLP Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-ja-${{ hashFiles('pyproject.toml') }}
          restore-keys: pip-ja-

      - name: Cache Stanza Japanese model
        uses: actions/cache@v4
        with:
          path: ~/stanza_resources
          key: stanza-ja-v1
          restore-keys: stanza-ja-

      - name: Install Japanese NLP deps
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,ja]"

      - name: Download Stanza Japanese model
        run: |
          python -c "import stanza; stanza.download('ja', verbose=False)"

      - name: Run Japanese tests
        run: |
          pytest tests/test_ja_pos.py tests/test_ja_pos_map.py -v --tb=short

  #  CLI integration tests (all languages) 
  test-cli:
    name: CLI Tests (All Tools)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-cli-all-${{ hashFiles('pyproject.toml') }}
          restore-keys: pip-cli-all-

      - name: Cache NLP models
        uses: actions/cache@v4
        with:
          path: |
            ~/stanza_resources
            ~/.cache/spacy
          key: nlp-models-cli-v1
          restore-keys: nlp-models-cli-

      - name: Install all dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,en,fa,ja]"
          python -m spacy download en_core_web_sm

      - name: Download NLTK data
        run: python -c "import nltk; nltk.download('averaged_perceptron_tagger_eng'); nltk.download('cmudict')"

      - name: Download Stanza models
        run: |
          python -c "import stanza; stanza.download('fa', verbose=False)"
          python -c "import stanza; stanza.download('ja', verbose=False)"

      - name: Run CLI tests
        run: |
          pytest tests/test_cli.py -v --tb=short

  #  Full coverage report
  coverage:
    name: Coverage Report
    runs-on: ubuntu-latest
    needs: [test-core, test-web, test-nlp-en, test-nlp-fa, test-nlp-ja, test-cli]
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-coverage-${{ hashFiles('pyproject.toml') }}
          restore-keys: pip-coverage-

      - name: Cache NLP models
        uses: actions/cache@v4
        with:
          path: |
            ~/stanza_resources
            ~/.cache/spacy
          key: nlp-models-coverage-v1
          restore-keys: nlp-models-

      - name: Install all dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,en,fa,ja]"
          pip install flask flask-cors markupsafe
          python -m spacy download en_core_web_sm

      - name: Download NLTK data
        run: python -c "import nltk; nltk.download('averaged_perceptron_tagger_eng'); nltk.download('cmudict')"

      - name: Download Stanza models
        run: |
          python -c "import stanza; stanza.download('fa', verbose=False)"
          python -c "import stanza; stanza.download('ja', verbose=False)"

      - name: Run ALL tests with coverage
        run: |
          pytest tests/ -v --cov=lexprep --cov-report=xml --cov-report=term-missing --tb=short

      - name: Upload coverage to Codecov
        if: github.event_name == 'push'
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          fail_ci_if_error: false
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
